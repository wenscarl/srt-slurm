# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

"""Tests for endpoint allocation logic."""

import pytest

from srtctl.core.topology import allocate_endpoints, endpoints_to_processes


class TestAllocateEndpoints:
    """Tests for allocate_endpoints function."""

    def test_multiple_endpoints_per_node(self):
        """Test multiple endpoints sharing a single node."""
        # 2 prefill endpoints, 2 GPUs each, 4 GPUs per node -> both on node0
        # 2 decode endpoints, 2 GPUs each, 4 GPUs per node -> both on node1
        endpoints = allocate_endpoints(
            num_prefill=2,
            num_decode=2,
            num_agg=0,
            gpus_per_prefill=2,
            gpus_per_decode=2,
            gpus_per_agg=8,
            gpus_per_node=4,
            available_nodes=("node0", "node1"),
        )

        assert len(endpoints) == 4

        # Check prefill endpoints - both should be on the SAME node
        prefill_eps = [e for e in endpoints if e.mode == "prefill"]
        assert len(prefill_eps) == 2
        assert prefill_eps[0].nodes[0] == prefill_eps[1].nodes[0] == "node0"
        assert prefill_eps[0].total_gpus == 2
        assert prefill_eps[1].total_gpus == 2
        # They should have different GPU indices
        assert prefill_eps[0].gpu_indices != prefill_eps[1].gpu_indices
        assert prefill_eps[0].gpu_indices == frozenset({0, 1})
        assert prefill_eps[1].gpu_indices == frozenset({2, 3})

        # Check decode endpoints - both should be on the SAME node (node1)
        decode_eps = [e for e in endpoints if e.mode == "decode"]
        assert len(decode_eps) == 2
        assert decode_eps[0].nodes[0] == decode_eps[1].nodes[0] == "node1"
        assert decode_eps[0].gpu_indices == frozenset({0, 1})
        assert decode_eps[1].gpu_indices == frozenset({2, 3})

    def test_full_node_endpoints(self):
        """Test endpoints that use full nodes."""
        endpoints = allocate_endpoints(
            num_prefill=2,
            num_decode=2,
            num_agg=0,
            gpus_per_prefill=4,
            gpus_per_decode=4,
            gpus_per_agg=8,
            gpus_per_node=4,
            available_nodes=("node0", "node1", "node2", "node3"),
        )

        assert len(endpoints) == 4

        # Each endpoint should use a full node
        for ep in endpoints:
            assert ep.total_gpus == 4
            assert len(ep.nodes) == 1

    def test_multi_node_endpoints(self):
        """Test endpoints that span multiple nodes."""
        # 1 prefill worker, 8 GPUs, 4 GPUs per node -> spans 2 nodes
        endpoints = allocate_endpoints(
            num_prefill=1,
            num_decode=1,
            num_agg=0,
            gpus_per_prefill=8,
            gpus_per_decode=8,
            gpus_per_agg=8,
            gpus_per_node=4,
            available_nodes=("node0", "node1", "node2", "node3"),
        )

        assert len(endpoints) == 2

        # Each endpoint should span 2 nodes
        for ep in endpoints:
            assert len(ep.nodes) == 2
            assert ep.total_gpus == 8

    def test_insufficient_gpus(self):
        """Test that we raise an error when there are insufficient GPUs."""
        # This should raise an IndexError when trying to access nodes that don't exist
        with pytest.raises((ValueError, IndexError)):
            allocate_endpoints(
                num_prefill=2,
                num_decode=2,
                num_agg=0,
                gpus_per_prefill=8,
                gpus_per_decode=8,
                gpus_per_agg=8,
                gpus_per_node=4,
                available_nodes=("node0", "node1"),  # Only 8 GPUs total, need 32
            )

    def test_single_endpoint_single_gpu(self):
        """Test edge case: single endpoint with single GPU."""
        endpoints = allocate_endpoints(
            num_prefill=1,
            num_decode=0,
            num_agg=0,
            gpus_per_prefill=1,
            gpus_per_decode=1,
            gpus_per_agg=8,
            gpus_per_node=4,
            available_nodes=("node0",),
        )

        assert len(endpoints) == 1
        assert endpoints[0].mode == "prefill"
        assert endpoints[0].total_gpus == 1

    def test_aggregated_mode(self):
        """Test aggregated mode (no disaggregation)."""
        endpoints = allocate_endpoints(
            num_prefill=0,
            num_decode=0,
            num_agg=2,
            gpus_per_prefill=4,
            gpus_per_decode=4,
            gpus_per_agg=4,
            gpus_per_node=4,
            available_nodes=("node0", "node1"),
        )

        assert len(endpoints) == 2
        for ep in endpoints:
            assert ep.mode == "agg"
            assert ep.total_gpus == 4


class TestEndpointsToProcesses:
    """Tests for endpoints_to_processes function."""

    def test_process_construction(self):
        """Test that endpoints_to_processes creates correct process mappings."""
        endpoints = allocate_endpoints(
            num_prefill=1,
            num_decode=1,
            num_agg=0,
            gpus_per_prefill=2,
            gpus_per_decode=2,
            gpus_per_agg=8,
            gpus_per_node=4,
            available_nodes=("node0",),
        )

        processes = endpoints_to_processes(endpoints, base_sys_port=8081)

        # SGLang creates one process per node
        assert len(processes) == 2

        # Check all sys_ports are unique
        ports = [p.sys_port for p in processes]
        assert len(ports) == len(set(ports)), "All processes should have unique sys_ports"

        # Check http_ports are unique per node (both on node0, so should differ)
        http_ports = [p.http_port for p in processes]
        assert len(http_ports) == len(set(http_ports)), "Processes on same node should have unique http_ports"

    def test_multi_node_process_construction(self):
        """Test process construction for multi-node endpoints."""
        endpoints = allocate_endpoints(
            num_prefill=1,
            num_decode=0,
            num_agg=0,
            gpus_per_prefill=8,
            gpus_per_decode=4,
            gpus_per_agg=8,
            gpus_per_node=4,
            available_nodes=("node0", "node1"),
        )

        processes = endpoints_to_processes(endpoints, base_sys_port=8081)

        # Multi-node endpoint should create one process per node
        assert len(processes) == 2
        nodes = [p.node for p in processes]
        assert "node0" in nodes
        assert "node1" in nodes

        # Only leader gets http_port, child gets 0
        leader = [p for p in processes if p.is_leader][0]
        assert leader.http_port == 30000
        assert leader.bootstrap_port == 31000  # prefill gets bootstrap port

        child = [p for p in processes if not p.is_leader][0]
        assert child.http_port == 0
        # All processes in prefill endpoint share the same bootstrap port
        assert child.bootstrap_port == leader.bootstrap_port

    def test_cuda_visible_devices(self):
        """Test that CUDA_VISIBLE_DEVICES is set correctly for each process."""
        endpoints = allocate_endpoints(
            num_prefill=2,
            num_decode=0,
            num_agg=0,
            gpus_per_prefill=2,
            gpus_per_decode=2,
            gpus_per_agg=8,
            gpus_per_node=4,
            available_nodes=("node0",),
        )

        processes = endpoints_to_processes(endpoints, base_sys_port=8081)

        # Each process should have correct GPU indices
        for p in processes:
            assert len(p.gpu_indices) == 2
            # Check cuda_visible_devices is formatted correctly
            assert "," in p.cuda_visible_devices or p.cuda_visible_devices.isdigit()

    def test_kv_events_port_allocation(self):
        """Test that kv_events_port is allocated for all worker leaders."""
        endpoints = allocate_endpoints(
            num_prefill=2,
            num_decode=2,
            num_agg=0,
            gpus_per_prefill=2,
            gpus_per_decode=2,
            gpus_per_agg=8,
            gpus_per_node=4,
            available_nodes=("node0", "node1"),
        )

        processes = endpoints_to_processes(endpoints, base_sys_port=8081)

        # Get all leaders (they should have kv_events_port)
        leaders = [p for p in processes if p.is_leader]
        assert len(leaders) == 4  # 2 prefill + 2 decode

        # All leaders should have globally unique kv_events_port
        kv_ports = [p.kv_events_port for p in leaders]
        assert all(port is not None for port in kv_ports)
        assert len(kv_ports) == len(set(kv_ports)), "All kv_events_ports should be globally unique"

        # Ports should be sequential starting from 5550
        assert sorted(kv_ports) == [5550, 5551, 5552, 5553]

        # Non-leaders should not have kv_events_port
        non_leaders = [p for p in processes if not p.is_leader]
        for p in non_leaders:
            assert p.kv_events_port is None

    def test_kv_events_port_same_node_unique(self):
        """Test kv_events_port is unique even when workers share a node."""
        # 2 prefill workers on same node
        endpoints = allocate_endpoints(
            num_prefill=2,
            num_decode=0,
            num_agg=0,
            gpus_per_prefill=2,
            gpus_per_decode=2,
            gpus_per_agg=8,
            gpus_per_node=4,
            available_nodes=("node0",),
        )

        processes = endpoints_to_processes(endpoints, base_sys_port=8081)

        # Both on node0, both should have unique ports
        assert len(processes) == 2
        assert processes[0].node == processes[1].node == "node0"
        assert processes[0].kv_events_port != processes[1].kv_events_port
        assert processes[0].kv_events_port == 5550
        assert processes[1].kv_events_port == 5551
