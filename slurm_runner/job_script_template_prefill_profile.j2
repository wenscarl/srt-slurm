#!/bin/bash
#SBATCH --job-name={{ job_name }}
#SBATCH --nodes={{ prefill_nodes }}
#SBATCH --ntasks={{ prefill_nodes }}
#SBATCH --ntasks-per-node=1
#SBATCH --account={{ account }}
#SBATCH --time={{ time_limit }}
#SBATCH --output={{ log_dir_prefix }}/%j_{{ prefill_workers }}P_profile_{{ timestamp }}/log.out
#SBATCH --error={{ log_dir_prefix }}/%j_{{ prefill_workers }}P_profile_{{ timestamp }}/log.err
#SBATCH --partition={{ partition }}

# Prefill Profiling Job
set -x
PREFILL_NODES={{ prefill_nodes }}
PREFILL_WORKERS={{ prefill_workers }}
GPUS_PER_NODE={{ gpus_per_node }}
TOTAL_GPUS=$((PREFILL_NODES * GPUS_PER_NODE))
PREFILL_NODES_PER_WORKER=$((PREFILL_NODES / PREFILL_WORKERS))
{% if log_dir_prefix.startswith('/') %}
LOG_DIR="{{ log_dir_prefix }}/${SLURM_JOB_ID}_{{ prefill_workers }}P_profile_{{ timestamp }}"
{% else %}
LOG_DIR="${SLURM_SUBMIT_DIR}/{{ log_dir_prefix }}/${SLURM_JOB_ID}_{{ prefill_workers }}P_profile_{{ timestamp }}"
{% endif %}
SCRIPT_DIR="${SLURM_SUBMIT_DIR}/scripts"
OUTPUT_DIR="${SLURM_SUBMIT_DIR}/outputs"
MODEL_DIR="{{ model_dir }}"
CONFIG_DIR="{{ config_dir }}"
CONTAINER_IMAGE="{{ container_image }}"
NETWORK_INTERFACE="{{ network_interface }}"
GPU_TYPE="{{ gpu_type | default('h100') }}"
set +x

{% raw %}

mkdir -p "${OUTPUT_DIR}" "${LOG_DIR}"

# Source utility functions for robust IP discovery
source "${SCRIPT_DIR}/slurm_utils.sh"

nodes=($(scontrol show hostnames $SLURM_NODELIST))
if [ ${#nodes[@]} -ne $PREFILL_NODES ]; then
    echo "Error: Expected $PREFILL_NODES nodes but got ${#nodes[@]} nodes"
    exit 1
fi

# Print node information
for i in "${!nodes[@]}"; do
    echo "Node $i: ${nodes[$i]}"
done

MASTER_IP=$(get_node_ip "${nodes[0]}" "$SLURM_JOB_ID" "$NETWORK_INTERFACE")
if [ -z "$MASTER_IP" ]; then
    echo "Error: Could not retrieve IP address for master host ${nodes[0]}"
    exit 1
fi
echo "Master IP address: $MASTER_IP"

# Compute leader nodes for each worker
prefill_leaders=()
for i in $(seq 0 $((PREFILL_WORKERS - 1))); do
    leader_idx=$((i * PREFILL_NODES_PER_WORKER))
    prefill_leaders[$i]=$leader_idx
done

echo "Prefill worker leaders: ${prefill_leaders[@]}"

# Prepare enroot arguments to pass to srun commands
ENROOT_ARGS="\
    --container-image=${CONTAINER_IMAGE} \
    --no-container-entrypoint \
    --no-container-mount-home \
    --container-mounts=${MODEL_DIR}:/model/,${CONFIG_DIR}:/configs/,${SCRIPT_DIR}:/scripts/,${OUTPUT_DIR}:/outputs/,${LOG_DIR}:/logs/ \
"

# Build common worker arguments
{% endraw %}
SCRIPT_VARIANT="{{ script_variant }}"
{% raw %}
WORKER_ARGS="--gpu_type ${GPU_TYPE} --script-variant ${SCRIPT_VARIANT} --gpus_per_node ${GPUS_PER_NODE} --master_ip ${MASTER_IP}"
{% endraw %}
{% if use_init_location %}
{% raw %}
WORKER_ARGS="$WORKER_ARGS --use_init_locations"
{% endraw %}
{% endif %}
{% if use_dynamo_whls %}
{% raw %}
# Use dynamo wheels from configs
WORKER_ARGS="$WORKER_ARGS --use-dynamo-whls"
{% endraw %}
{% endif %}
{% raw %}

# Profiling mode: use sglang.launch_server instead of dynamo.sglang
WORKER_ARGS="$WORKER_ARGS --use-sglang-launch-server"

# Launch prefill workers
for worker_idx in $(seq 0 $((PREFILL_WORKERS - 1))); do
    leader_idx=${prefill_leaders[$worker_idx]}
    leader_node=${nodes[$leader_idx]}

    # Get leader IP for this worker group
    LEADER_IP=$(get_node_ip "$leader_node" "$SLURM_JOB_ID" "$NETWORK_INTERFACE")
    echo "Prefill worker $worker_idx leader: $leader_node ($LEADER_IP)"

    # Launch all nodes for this worker
    for node_idx in $(seq 0 $((PREFILL_NODES_PER_WORKER - 1))); do
        global_node_idx=$((leader_idx + node_idx))
        node=${nodes[$global_node_idx]}
        local_rank=$node_idx

        echo "Launching prefill worker $worker_idx, node $global_node_idx (local_rank $local_rank): $node"
        # No config dump during profiling
        cmd="srun --overlap $ENROOT_ARGS --nodes=1 --ntasks=1 --nodelist=$node --output=${LOG_DIR}/${node}_prefill_w${worker_idx}.out --error=${LOG_DIR}/${node}_prefill_w${worker_idx}.err python /scripts/worker_setup.py --leader_ip ${LEADER_IP} --worker_idx ${worker_idx} --local_rank ${local_rank} --nodes_per_worker ${PREFILL_NODES_PER_WORKER} --worker_type prefill --gpu_utilization_log /logs/${node}_prefill_w${worker_idx}_gpu_utilization.log ${WORKER_ARGS}"
        echo "$cmd"
        $cmd &
    done
done

echo ""
echo "To connect to the master prefill node:"
echo "srun $ENROOT_ARGS --jobid $SLURM_JOB_ID -w ${nodes[0]} --overlap --pty bash"

echo ""
echo "Make sure to cancel the job at the end:"
echo "scancel $SLURM_JOB_ID"

# Run prefill profiling
srun --nodes=1 --ntasks=1 $ENROOT_ARGS --jobid $SLURM_JOB_ID -w ${nodes[0]} --output=${LOG_DIR}/profile.out --error=${LOG_DIR}/profile.err --overlap bash /scripts/profile.sh prefill &

{% endraw %}

{% raw %}
wait -n
first_exit_code=$?
echo "Script finished at $(date) with exit code ${first_exit_code}"
exit $first_exit_code
{% endraw %}
