# Example: 4P1D with sglang source code debugging enabled
# Usage: srtctl apply -f recipies/gb200-fp4/1k1k/max-tpt-debug.yaml

name: "gb200-fp4-max-tpt-debug"

model:
  path: "dsfp4"
  container: "0.5.5.post2"
  precision: "fp4"

resources:
  gpu_type: "gb200"
  prefill_nodes: 4
  decode_nodes: 12
  prefill_workers: 1
  decode_workers: 1
  gpus_per_node: 4

backend:
  use_sglang_router: "true"
  
  # Optional: Mount sglang source code for debugging
  # If specified, sglang will be installed from source using: pip install -e . --no-deps
  # The source directory will be mounted at /ext-sglang-src/ in the container
  # Example: sglang_src_dir: "/path/to/your/sglang"
  # If not specified or left empty, the container's pre-installed sglang will be used
  sglang_src_dir: "/mnt/lustre01/users/slurm-shared/shuw/sglang"
  
  # Prefill-specific environment variables
  prefill_environment:
    TORCH_DISTRIBUTED_DEFAULT_TIMEOUT: "1800"
    PYTHONUNBUFFERED: "1"
    DYN_SKIP_SGLANG_LOG_FORMATTING: "1"
    SGLANG_NVFP4_CKPT_FP8_GEMM_IN_ATTN: "1"
    SGLANG_PER_TOKEN_GROUP_QUANT_8BIT_V2: "1"
    SGLANG_DISAGGREGATION_HEARTBEAT_MAX_FAILURE: "100000"
    SGLANG_DISAGGREGATION_BOOTSTRAP_TIMEOUT: "100000"
    SGLANG_DISAGGREGATION_WAITING_TIMEOUT: "100000"
    SGLANG_HACK_SEQ_BOOTSTRAP_ROOM: "1"
    MC_TE_METRIC: "true"
    MC_FORCE_MNNVL: "1"
    NCCL_MNNVL_ENABLE: "1"
    NCCL_CUMEM_ENABLE: "1"
    SGLANG_MOONCAKE_CUSTOM_MEM_POOL: "True"
    SGLANG_USE_MESSAGE_QUEUE_BROADCASTER: "0"
    SGLANG_DISABLE_TP_MEMORY_INBALANCE_CHECK: "1"
    SGLANG_LOG_FORWARD_ITERS: "1"

  # Decode-specific environment variables
  decode_environment:
    TORCH_DISTRIBUTED_DEFAULT_TIMEOUT: "1800"
    PYTHONUNBUFFERED: "1"
    DYN_SKIP_SGLANG_LOG_FORMATTING: "1"
    SGLANG_NVFP4_CKPT_FP8_GEMM_IN_ATTN: "1"
    SGLANG_PER_TOKEN_GROUP_QUANT_8BIT_V2: "1"
    SGLANG_DISAGGREGATION_HEARTBEAT_MAX_FAILURE: "100000"
    SGLANG_DISAGGREGATION_BOOTSTRAP_TIMEOUT: "100000"
    SGLANG_DISAGGREGATION_WAITING_TIMEOUT: "100000"
    SGLANG_HACK_SEQ_BOOTSTRAP_ROOM: "1"
    MC_TE_METRIC: "true"
    MC_FORCE_MNNVL: "1"
    NCCL_MNNVL_ENABLE: "1"
    NCCL_CUMEM_ENABLE: "1"
    SGLANG_MOONCAKE_CUSTOM_MEM_POOL: "True"
    SGLANG_USE_MESSAGE_QUEUE_BROADCASTER: "0"
    SGLANG_DISABLE_TP_MEMORY_INBALANCE_CHECK: "1"
    SGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK: "1024"
    SGLANG_CUTEDSL_MOE_NVFP4_DISPATCH: "1"
    SGLANG_MOE_NVFP4_DISPATCH: "1"
    SGLANG_FLASHINFER_FP4_GEMM_BACKEND: "cutlass"
    SGLANG_LOG_FORWARD_ITERS: "1"
   # SGLANG_BLOCK_NONZERO_RANK_CHILDREN: "0"


  sglang_config:
    prefill:
      # Model configuration
      served-model-name: "deepseek-ai/DeepSeek-R1"
      model-path: "/model/"
      trust-remote-code: true

      # KV cache and attention
      kv-cache-dtype: "fp8_e4m3"
      attention-backend: "trtllm_mla"

      # Quantization
      quantization: "modelopt_fp4"
      moe-runner-backend: "flashinfer_cutlass"

      # Radix cache disabled
      disable-radix-cache: true
      disable-chunked-prefix-cache: true

      # Other flags
      stream-interval: 50
      decode-log-interval: 1000
      watchdog-timeout: 1000000
      context-length: 2176
      disable-shared-experts-fusion: true
      eplb-algorithm: "deepseek"
      #enable-eplb: true
      enable-expert-distribution-metrics: true
      #expert-distribution-recorder-mode: "stat"
      #eplb-rebalance-num-iterations: 100
      deepep-config: "/configs/deepep_config.json"
      disaggregation-bootstrap-port: 30001

      # Prefill-specific mode
      disaggregation-mode: "prefill"

      # Memory and token limits
      mem-fraction-static: 0.84
      max-total-tokens: 131072
      max-prefill-tokens: 16384 #32768
      #chunked-prefill-size: 65536
      enable-single-batch-overlap: true

      # Request handling
      max-running-requests: 5632
      load-balance-method: "round_robin"

      # Performance optimizations
      disable-cuda-graph: true
      enable-dp-attention: true

      # Parallelism
      tp-size: 4
      dp-size: 4
      ep-size: 4

    decode:
      # Model configuration
      served-model-name: "deepseek-ai/DeepSeek-R1"
      model-path: "/model/"
      trust-remote-code: true

      # KV cache and attention
      kv-cache-dtype: "fp8_e4m3"
      attention-backend: "trtllm_mla"

      # Quantization
      quantization: "modelopt_fp4"
      moe-runner-backend: "flashinfer_cutedsl"

      # Radix cache disabled
      disable-radix-cache: true
      disable-chunked-prefix-cache: true

      # Other flags
      stream-interval: 50
      decode-log-interval: 1000
      watchdog-timeout: 1000000
      context-length: 2176
      disable-shared-experts-fusion: true
      eplb-algorithm: "deepseek"
      #enable-eplb: true
      #expert-distribution-recorder-mode: "stat"
      #eplb-rebalance-num-iterations: 100
      #enable-expert-distribution-metrics: true
      deepep-config: "/configs/deepep_config.json"
      disaggregation-bootstrap-port: 30001

      # Decode-specific mode
      disaggregation-mode: "decode"

      # Memory and token limits
      mem-fraction-static: 0.83
      max-total-tokens: 3122380 #131072
      chunked-prefill-size: 786432 #65536

      # Request handling
      max-running-requests: 67854 # 49152
      enable-single-batch-overlap: true

      # DeepEP configuration
      moe-a2a-backend: "deepep"
      deepep-mode: "low_latency"
      ep-dispatch-algorithm: "static"
      ep-num-redundant-experts: 32

      # CUDA graphs (extensive batch size list)
      cuda-graph-bs: [1408]
      num-reserved-decode-tokens: 112

      # Additional decode optimizations
      moe-dense-tp-size: 1
      enable-dp-lm-head: true
      prefill-round-robin-balance: true
      enable-dp-attention: true

      # Parallelism
      tp-size: 48
      dp-size: 48
      ep-size: 48

benchmark:
  type: "sa-bench"
  isl: 1024
  osl: 1024
  concurrencies: "12000"
  req_rate: "inf"
